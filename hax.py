# -*- coding:utf-8 -*-
# -------------------------------
# @Author : github@wh1te3zzz https://github.com/wh1te3zzz/hax
# @Time : 2025-05-15 13:57:56
# haxç›‘æ§è„šæœ¬ï¼Œç›‘æ§æ•°æ®ä¸­å¿ƒå˜åŒ–å’Œå½“å‰å¯åˆ›å»ºçš„åŒºåŸŸï¼Œä¸å¯ç”¨é€šçŸ¥ç‰ˆæœ¬
# -------------------------------
"""
hax ç›‘æ§

cron: 59 * * * *
const $ = new Env("hax ç›‘æ§");
"""
import re
import logging

import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å¸¸é‡å®šä¹‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36",
    "Content-Type": "application/json",
}

URL_HAX_SERVER_INFO = "https://hax.co.id/data-center"
URL_HAX_CREATE_VPS = "https://hax.co.id/create-vps"
URL_WOIDEN_CREATE_VPS = "https://woiden.id/create-vps"


class Hax:
    @staticmethod
    def fetch_page(url: str) -> str:
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logging.error(f"è¯·æ±‚å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
            return ""

    def parse_server_info(self, html_text: str) -> dict:
        """è§£ææœåŠ¡å™¨ä¿¡æ¯ï¼ˆåŒºåŸŸä¸æ•°é‡ï¼‰"""
        soup = BeautifulSoup(html_text, "html.parser")
        zone_list = [x.text for x in soup("h5", class_="card-title mb-4")]
        sum_list = [x.text for x in soup("h1", class_="card-text")]

        result = {}
        for zone_info, count_info in zip(zone_list, sum_list):
            parts = zone_info.split("-", 1)
            region = parts[0].lstrip("./")
            suffix = f"{parts[1]}({count_info.rstrip(' VPS')}â™)" if len(parts) > 1 else count_info

            result.setdefault(region, []).append(suffix)

        return result

    def get_server_info(self) -> str:
        """è·å–å¹¶æ ¼å¼åŒ–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
        html_text = self.fetch_page(URL_HAX_SERVER_INFO)
        if not html_text:
            return ""

        info_dict = self.parse_server_info(html_text)
        lines = [f">>{region}-" + ", ".join(values) + "\n" for region, values in info_dict.items()]
        return "".join(lines)

    def parse_vps_centers(self, html_text: str, vir: bool = False) -> str:
        """è§£æ VPS åŒºåŸŸé€‰é¡¹"""
        soup = BeautifulSoup(html_text, "html.parser")
        options = soup.find_all("option", value=re.compile(r"^[A-Z]{2,}-"))
        centers = [opt.text for opt in options]

        if vir:
            processed = [(c.split(" (")[1].rstrip(")"), c.split(" (")[0]) for c in centers if " (" in c]
            result_dict = {}
            for key, val in processed:
                result_dict.setdefault(key, []).append(val)
            return "".join([f"â˜…{k}â˜… " + ", ".join(v) + "\n" for k, v in result_dict.items()])

        return "\n".join(centers)

    def get_data_center(self, url: str, vir: bool = False) -> str:
        """è·å–æ•°æ®ä¸­å¿ƒä¿¡æ¯"""
        html_text = self.fetch_page(url)
        if not html_text:
            return ""
        return self.parse_vps_centers(html_text, vir)

    def main(self) -> str:
        hax_str = self.get_server_info()
        hax_stat = f"[ğŸ›°Hax Stats / Hax å¼€é€šæ•°æ®]\n{hax_str}\n"

        vir_str = self.get_data_center(URL_HAX_CREATE_VPS, vir=True)
        woiden_str = self.get_data_center(URL_WOIDEN_CREATE_VPS)

        data_center = (
            f"[ğŸš©Available Centers / å¯å¼€é€šåŒºåŸŸ]\n"
            f'---------- <a href="{URL_HAX_CREATE_VPS}">Hax</a> ----------\n'
            f"{vir_str}"
            f'---------- <a href="{URL_WOIDEN_CREATE_VPS}">Woiden</a> ----------\n'
            f"{woiden_str}\n"
        )

        return hax_stat + data_center


if __name__ == "__main__":
    hax = Hax()
    try:
        result = hax.main()
        logging.info(result)
    except Exception as e:
        logging.error(f"æ‰§è¡Œä¸»ç¨‹åºå‡ºé”™: {e}")
