# -*- coding:utf-8 -*-
# -------------------------------
# @Author : github@wh1te3zzz https://github.com/wh1te3zzz/hax
# @Time : 2025-05-15 13:57:56
# haxç›‘æ§è„šæœ¬ï¼Œç›‘æ§æ•°æ®ä¸­å¿ƒå˜åŒ–å’Œå½“å‰å¯åˆ›å»ºçš„åŒºåŸŸ
# -------------------------------
"""
hax å·²å¼€é€šæ•°æ®

cron: 59 * * * *
const $ = new Env("hax å·²å¼€é€šæ•°æ®");
"""
# script_monitor_hax_stats.py

import requests
from bs4 import BeautifulSoup

from cache_utils import load_last_data, save_current_data
from wxpusher_notifier import wx_pusher_notify

HEADERS = {
    "User-Agent": "Mozilla/5.0",
}

URL_HAX_SERVER_INFO = "https://hax.co.id/data-center"
CACHE_KEY = "hax_server_info"


class HaxStatsMonitor:
    @staticmethod
    def fetch_page(url):
        """
        è¯·æ±‚é¡µé¢å†…å®¹ã€‚
        
        :param url: ç›®æ ‡URL
        :return: é¡µé¢å†…å®¹æˆ–ç©ºå­—ç¬¦ä¸²ï¼ˆè¯·æ±‚å¤±è´¥æ—¶ï¼‰
        """
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            res.raise_for_status()
            return res.text
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return ""

    def parse_server_info(self, html_text):
        """
        è§£ææœåŠ¡å™¨å¼€é€šæ•°æ®ã€‚
        
        :param html_text: é¡µé¢HTMLå†…å®¹
        :return: è§£æåçš„å†…å®¹å­—ç¬¦ä¸²
        """
        soup = BeautifulSoup(html_text, "html.parser")
        zone_list = [x.text for x in soup("h5", class_="card-title mb-4")]
        sum_list = [x.text for x in soup("h1", class_="card-text")]

        result = {}
        for zone_info, count_info in zip(zone_list, sum_list):
            parts = zone_info.split("-", 1)
            region = parts[0].lstrip("./")
            suffix = f"{parts[1]}({count_info.rstrip(' VPS')}â™)" if len(parts) > 1 else count_info
            result.setdefault(region, []).append(suffix)

        lines = [f">>{region}-" + ", ".join(values) + "\n" for region, values in result.items()]
        return "".join(lines)

    def get_server_info(self):
        """
        è·å–æœåŠ¡å™¨å¼€é€šæ•°æ®ã€‚
        
        :return: è§£æåçš„æœåŠ¡å™¨å¼€é€šæ•°æ®
        """
        html = self.fetch_page(URL_HAX_SERVER_INFO)
        if not html:
            return ""
        return self.parse_server_info(html)

    def main(self):
        current_data = self.get_server_info()
        if not current_data:
            print("è·å–æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æœ¬æ¬¡æ£€æŸ¥ã€‚")
            return

        last_data = load_last_data(CACHE_KEY)
        if current_data != last_data:
            print("æ£€æµ‹åˆ°æ•°æ®å˜åŒ–ï¼Œå‡†å¤‡æ¨é€é€šçŸ¥...")
            wx_pusher_notify(f"[ğŸ›°Hax Stats / Hax å·²å¼€é€šæ•°æ®]\n{current_data}")
            save_current_data(CACHE_KEY, current_data)
        else:
            print("æ•°æ®æœªå‘ç”Ÿå˜åŒ–ï¼Œä¸æ¨é€é€šçŸ¥ã€‚")


if __name__ == "__main__":
    monitor = HaxStatsMonitor()
    monitor.main()
